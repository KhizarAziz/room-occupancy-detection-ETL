{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "i7fid5VD9Oxf",
        "edJsrKaJhCvt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUMN0PxGJ1Wk"
      },
      "outputs": [],
      "source": [
        "!gdown 1k-uSHaMEagMkORIRr2oJ0dtOfFcbyLHD\n",
        "!unzip occupancy_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import cross_val_score, learning_curve"
      ],
      "metadata": {
        "id": "iRcMv54_KKs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/datatest.txt',sep=',')\n",
        "df2 = pd.read_csv('/content/datatest2.txt',sep=',')\n",
        "df_training = pd.read_csv('/content/datatraining.txt',sep=',')"
      ],
      "metadata": {
        "id": "i8ufFMYgKVjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Let's Explore the Data**"
      ],
      "metadata": {
        "id": "i7fid5VD9Oxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data info:**\n",
        "* date time year-month-day hour:minute:second\n",
        "* Temperature, in Celsius\n",
        "* Relative Humidity, %\n",
        "* Light, in Lux\n",
        "* CO2, in ppm\n",
        "* Humidity Ratio, Derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air\n",
        "* Occupancy, 0 or 1, 0 for not occupied, 1 for occupied status"
      ],
      "metadata": {
        "id": "ZfjjXwViK_-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape, df2.shape , df_training.shape"
      ],
      "metadata": {
        "id": "jNII5MnKKygz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info(),df2.info(), df_training.info()"
      ],
      "metadata": {
        "id": "p96tWY1OL-mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the difference between the given files. Try to understand why is there 3 files and not single!"
      ],
      "metadata": {
        "id": "dytm5B4mL_Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "tPdnZo1Ffm5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_training.head()"
      ],
      "metadata": {
        "id": "UkqSP3W1fw_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head()"
      ],
      "metadata": {
        "id": "BQoiTZtvf0Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the timeline of the data"
      ],
      "metadata": {
        "id": "fgH0QgNVOJKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"dataset.txt is within {df.date.min()} - {df.date.max()} with entries : {df.size}\")\n",
        "print(f\"datatraining.txt is within {df_training.date.min()} - {df_training.date.max()} with entries : {df.size}\")\n",
        "print(f\"dataset2.txt is within {df2.date.min()} - {df2.date.max()} with entries : {df2.size}\")"
      ],
      "metadata": {
        "id": "twIEyig_L9mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing useful so far!"
      ],
      "metadata": {
        "id": "QyBtBkm1MYcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like these are 1 data splited into 3 parts!\n",
        "Let's join them all and see what do we have"
      ],
      "metadata": {
        "id": "9j80Cnh3Nus2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat([df,df2,df_training]).reset_index(drop=True)\n",
        "merged_df.shape"
      ],
      "metadata": {
        "id": "pFCRDxuVNAx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "id": "x8C9tywcOICp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Time Series Analysis**"
      ],
      "metadata": {
        "id": "YxZpySNuUU9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix date type\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'])"
      ],
      "metadata": {
        "id": "5cwgJAGxOUw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame and 'date_column' is your datetime column\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "merged_df['hour'] = merged_df['date'].dt.hour\n",
        "\n",
        "# Aggregate data\n",
        "hourly_data = merged_df.groupby(['hour', 'Occupancy']).size().unstack()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "hourly_data.plot(kind='bar', stacked=False, color=['blue', 'orange'], ax=plt.gca())\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Occupancy Sum')  # Adjust label as needed\n",
        "plt.title('24-Hour Seasonality Pattern')\n",
        "plt.xticks(range(0, 24))  # Optional: to show all hours\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KL0W4-LlOnL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming merged_df is your DataFrame and 'date' is your datetime column\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "merged_df['day_of_week'] = merged_df['date'].dt.day_name()  # Extracts the day of the week\n",
        "\n",
        "# Count the occurrences of each occupancy value for each day\n",
        "occupancy_count = merged_df.groupby(['day_of_week', 'Occupancy']).size().unstack()\n",
        "\n",
        "# Reorder days if necessary\n",
        "order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "occupancy_count = occupancy_count.reindex(order)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "occupancy_count.plot(kind='bar', stacked=False, color=['blue', 'orange'], ax=plt.gca())\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Count of Occupancy')\n",
        "plt.title('Occupancy Count by Day of the Week')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IStM2MUXSPQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Statistical Analysis**"
      ],
      "metadata": {
        "id": "12yf6qe1Uami"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.describe().T"
      ],
      "metadata": {
        "id": "odyk8XaRUc6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.describe(include='object').T"
      ],
      "metadata": {
        "id": "pgZ1Mqj2yOqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "columns_to_plot = ['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']\n",
        "\n",
        "# Plotting each feature in a separate subplot\n",
        "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 10))\n",
        "axes = axes.flatten()  # Flatten the array of axes\n",
        "\n",
        "for i, col in enumerate(columns_to_plot):\n",
        "    merged_df.boxplot(column=col, ax=axes[i])\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o9xxdDev5SAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "columns_to_plot = ['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 15))  # Adjust grid size as needed\n",
        "axes = axes.flatten()  # Flatten the array of axes\n",
        "\n",
        "for i, column in enumerate(columns_to_plot):\n",
        "    merged_df.boxplot(column=column, by='Occupancy', ax=axes[i])\n",
        "    axes[i].set_title(f'{column} by Occupancy')\n",
        "    axes[i].set_xlabel('Occupancy')\n",
        "    axes[i].set_ylabel(column)\n",
        "\n",
        "plt.suptitle('Box Plots of Variables by Occupancy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qz2UDjsxXlFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Distribution"
      ],
      "metadata": {
        "id": "2Oj9C5GrgHVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Occupancy'].plot(kind='hist', bins=3, rwidth=0.8, density=False)"
      ],
      "metadata": {
        "id": "3VJYhGTOf2eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame and 'Occupancy' is your binary column\n",
        "label_counts = df['Occupancy'].value_counts()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightgreen'])\n",
        "plt.title('Distribution of Binary Label')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular\n",
        "\n",
        "# Adding a legend for clarity\n",
        "plt.legend(['Occupied', 'Unoccupied'], title=\"Labels\", loc=\"best\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sDVIjgWs1ol9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df[merged_df.duplicated()].shape"
      ],
      "metadata": {
        "id": "fP-MITY83vI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**"
      ],
      "metadata": {
        "id": "edJsrKaJhCvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# date column not needed anymore\n",
        "merged_df.drop('date',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "iLxanbbFBc6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.describe()"
      ],
      "metadata": {
        "id": "MGgRypKXCaN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Encoding and scaling"
      ],
      "metadata": {
        "id": "S_OWnSMTCa_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding day_of_week\n",
        "day_to_num = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
        "merged_df['day_of_week'] = merged_df['day_of_week'].map(day_to_num)"
      ],
      "metadata": {
        "id": "LYM_a7QejUY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling\n",
        "# Assuming df is your DataFrame\n",
        "features = ['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio', 'Occupancy',\n",
        "       'hour']\n",
        "\n",
        "scaled_df = merged_df.copy()\n",
        "# Initialize the RobustScaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Applying the scaler to the necessary columns in the dataset\n",
        "scaled_df[features] = scaler.fit_transform(scaled_df[features])"
      ],
      "metadata": {
        "id": "EollzXq4CkkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = scaled_df.corr()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BmUNLGhP-0Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose to use PCA dim reduction in order to avoid multi collinearity"
      ],
      "metadata": {
        "id": "MecUc-RV3IAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying PCA\n",
        "pca = PCA()\n",
        "pca.fit(scaled_df)\n",
        "\n",
        "# Calculating the explained variance ratio for each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plotting the Cumulative Explained Variance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--')\n",
        "plt.title('Explained Variance by Different Principal Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5RFK4OTpBLKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Elbow method, we can see 3 would be a good number of PCs for capturing the most variance in the data.\n",
        "Let's apply PCA to data"
      ],
      "metadata": {
        "id": "-TSdPQgsKGkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PCA with the desired number of components\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# Fit and transform the scaled data\n",
        "principalComponents = pca.fit_transform(scaled_df)\n",
        "\n",
        "# Creating a DataFrame with principal components\n",
        "principalDf = pd.DataFrame(data=principalComponents, columns=['PC'+str(i+1) for i in range(pca.n_components_)])\n",
        "\n",
        "# adding label back\n",
        "principalDf['Occupancy'] = scaled_df['Occupancy']"
      ],
      "metadata": {
        "id": "Mpb_dTVcCT3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "principalDf.head()"
      ],
      "metadata": {
        "id": "6r_Oend_KaEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "LwPJODeag0rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "ZiYzahd2RDQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation splitting\n",
        "X = principalDf.drop('Occupancy', axis=1)\n",
        "y = principalDf['Occupancy']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "u8X075dURNcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of models\n",
        "# Updated dictionary of models with added regularization parameters\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(C=1.0, penalty='l2', solver='liblinear'),  # L2 regularization\n",
        "    'RandomForest': RandomForestClassifier(max_depth=5),  # Limit the depth of trees\n",
        "    'GradientBoosting': GradientBoostingClassifier(max_depth=3),  # Limit the depth of trees\n",
        "    'SVM': SVC(C=1.0),  # Regularization parameter\n",
        "    'KNN': KNeighborsClassifier(),  # KNN doesn't have regularization but consider reducing k\n",
        "    'NaiveBayes': GaussianNB()  # Naive Bayes doesn't have regularization\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"{name} classification report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"{name} confusion matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(f\"{name} ROC AUC Score:\")\n",
        "    print(roc_auc_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "    print(f\"{name} Cross-validation scores: {cv_scores}\")\n",
        "\n",
        "    # Learning Curve\n",
        "    train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, cv=5)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "    plt.title(f\"Learning Curve for {name}\")\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "    print(\"-----\" * 10)"
      ],
      "metadata": {
        "id": "23Ra2pov41uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Choose a binary classification algorithm\n",
        "model = LogisticRegression()\n",
        "\n",
        "# 3. Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Evaluate the model's performance\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-HKF8UoERH41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56wFqbfg4jw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFMYoNliRT21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# data splitting (train test split)\n",
        "# prepare inputs(x) and outputs(y)\n",
        "# Selecting and training model\n",
        "# RandomForest (best starting candidate)\n"
      ],
      "metadata": {
        "id": "JkCbeaH8k7nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "luN3IIkrgmtD"
      }
    }
  ]
}